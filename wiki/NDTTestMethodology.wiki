#summary Description of the NDT test methodology

=NDT Test Methodology= 

== Abstract == 

The Network Diagnostic Tool (NDT) is a client/server program that provides network configuration and performance testing to a user's computer. NDT is designed to identify both performance problems and configuration problems. Performance problems affect the user experience, usually causing data transfers to take longer than expected. These problems are usually solved by tuning various TCP (Transmission Control Protocol) network parameters on the end host. Configuration problems also affect the user experience; however, tuning will not improve the end-to-end performance. The configuration fault must be found and corrected to change the end host behavior. NDT is providing enough information to accomplish these tasks. This document describes how these information is gathered and what NDT is and is not capable of answering.

== Table of Contents ==

<wiki:toc max_depth="3" />

== Introduction ==

NDT is a typical memory to memory client/server test device. Throughput measurements closely measure the network performance, and ignore disk I/O effects. The real strength is in the advanced diagnostic features that are enabled by the kernel data automatically collected by the web100 monitoring infrastructure.  This data is collected during the test (at 5 msec increments) and analyzed after the test completes to determine what, if anything, impacted the test. One of the MAJOR issues facing commodity Internet users is the performance limiting host configuration settings for the Windows XP operating system. To illustrate this, a cable modem user with basic service (15 Mbps download) would MAX out at 13 Mbps with a 40 msec RTT delay. Thus unless the ISP proxies content, the majority of traffic will be limited by the clients configuration and NOT the ISP's infrastructure.  The NDT server can detect and report this problem, saving consumers and ISP's dollars by allowing them to quickly identify where to start looking for a problem.

NDT operates on any client with a Java-enabled Web browser; further:
 * What it can do:
   * Positively state if Sender, Receiver, or Network is operating properly
   * Provide accurate application tuning info
   * Suggest changes to improve performance
 * What it can’t do:
   * Tell you where in the network the problem is
   * Tell you how other servers perform
   * Tell you how other clients will perform

== Performed tests ==

=== Middlebox Test ===

The middlebox test is a short throughput test from the server to the client with a limited CWND ([http://en.wikipedia.org/wiki/Congestion_window congestion window] - one of the factors that determines the number of bytes that can be outstanding at any time) to check for a duplex mismatch condition. Moreover, this test uses a pre-defined MSS value to check if any intermediate node will modify its connection settings.

A detailed description of all of the MID protocol messages can be found in the [NDTProtocol#Middlebox_test NDT Protocol document].

As a first step the server binds an ephemeral port and notify the client about this port number. The server also sets MSS on this port to 1456 (a strange value that it is unlikely a routers will have been tested with, so this also tests that they can handle such weird MSS sizes).

Next, the client connects to the server's ephemeral port. When the connection is successfully established, the server sets the maximum value of the congestion window for this connection to `2 * (The current maximum segment size (MSS))`.

In the next step the server starts a 5 seconds throughput test using the newly created connection. The NDT server sends packets as fast as possible (i.e. without any delays) during the test. These packets are written using the buffer of the following size: `(The current maximum segment size (MSS))`. If such buffer cannot be used, then the server uses a 8192 Byte one. The buffer contains a pre-generated pseudo random data (including only US-ASCII printable characters).

The server can temporarily stop sending packets when the following formula is fulfilled:
{{{
BUFFER_SIZE * 16 < ((Next Sequence Number To Be Sent) - (Oldest Unacknowledged Sequence Number) - 1)
}}}

The both `"Next Sequence Number To Be Sent"` and `"Oldest Unacknowledged Sequence Number"` values are obtained from the connection with the help of the [http://www.web100.org/ web100] library.

When the 5 seconds throughput test is over, the server sends the following results to the client:

|| CurMSS || The current maximum segment size (MSS), in octets.||
|| !WinScaleSent || The value of the transmitted window scale option if one was sent; otherwise, a value of -1. ||
|| !WinScaleRcvd || The value of the received window scale option if one was received; otherwise, a value of -1. ||

Next, the client sends its calculated throughput value to the server. The throughput value is calculated by taking the received bytes over the duration of the test. This value, in Bps, is then converted to kbps. This can be shown by the following formula:
{{{
THROUGHPUT_VALUE = (RECEIVED_BYTES / TEST_DURATION_SECONDS) * 8 / 1000
}}}

==== Known issues (Middlebox Test) ====

The formula used to find out when to temporarily stop sending packets is probably wrong. The idea was to use this as a part of the duplex mismatch detection, with a max of 2 packets in flight the Ethernet half-duplex sender should never see a collision so the throughput would be higher even though the buffer is limited.

However, the formula allows for more packets in flight and it uses the `"Next Sequence Number To Be Sent"` instead of the `"Maximum Value of Next Sequence Number To Be Sent"`. The difference between these values is that the first one is not monotonic (and thus not a counter) because TCP sometimes retransmits lost data by pulling the Next Sequence Number back to the missing data. The latter one is the farthest forward (right most or largest) value of the Next Sequence Number. These values are the same except the situation when the Next Sequence Number is pulled back during recovery.

=== Simple Firewall Test ===

The simple firewall test tries to find out any firewalls between the NDT client and the NDT server that will prevent connections to an ephemeral port numbers. The test is performed in both directions (i.e. the NDT client is trying to connect to the NDT server and the NDT server is trying to connect to the NDT client).

A detailed description of all of the SFW protocol messages can be found in the [NDTProtocol#Simple_firewall_test NDT Protocol document].

As a first step both NDT components (the server and the client) bind an ephemeral port and notifies the second component about this port number. In the second step both NDT components are executing in parallel:
 * The client is trying to connect to the server's ephemeral port and send a TEST_MSG message containing a pre-defined string "Simple firewall test" of length 20 using the newly created connection.
 * The server is trying to connect to the client's ephemeral port and send a TEST_MSG message containing a pre-defined string "Simple firewall test" of length 20 using the newly created connection.

Both client and server are waiting for a valid connection a limited amount of time. If the MaxRTT or MaxRTO is greater than 3 seconds, than the time limit in the SFW test is 3 seconds. Otherwise the time limit in the SFW test is 1 second.

The test is finished after the connection will be accepted or the time limit will be exceeded. If the time limit is exceeded, the firewall probably exists somewhere on the end-to-end path. If there is a connection and the pre-defined string is properly transferred, then there is probably no firewall on the end-to-end path (technically there still could be a firewall with a range of opened ports or a special rules that allowed this one particular connection to the ephemeral port). The third possibility is that there is a successful connection, but the expected pre-defined string is not transferred. This case does not adjudicate about the firewall existence.

In the last step the server sends its results to the client.

The possible simple firewall test result codes:

|| *Value* || *Description* ||
|| "0" || Test was not started/results were not received (this means an error condition like protocol error, which cannot happen during normal operation) ||
|| "1" || Test was successful (i.e. connection to the ephemeral port was possible and the pre-defined string was received) ||
|| "2" || There was a connection to the ephemeral port, but the pre-defined string was not received ||
|| "3" || There was no connection to the ephemeral port within the specified time ||

=== C2S Throughput Test ===

The C2S throughput test tests the achievable network throughput from the client to the server by performing a 10 seconds memory-to-memory data transfer.

A detailed description of all of the C2S protocol messages can be found in the [NDTProtocol#C2S_throughput_test NDT Protocol document].

As a first step the server binds a new port and notifies the client about this port number.

Next, the client connects to the server's newly bound port. When the connection is successfully established, the server initializes the following routines:
 * libpcap routines to perform packet trace used by the [NDTTestMethodology#Bottleneck_Link_Detection Bottleneck Link Detection] algorithm.
 * [NDTDataFormat#tcpdump_trace tcpdump trace] to dump all packets sent during the [NDTTestMethodology#C2S_Throughput_Test C2S throughput test] on the newly created connection. This tcpdump trace dump is only started when the `-t, --tcpdump` options are set.
 * [NDTDataFormat#web100_snaplog_trace web100 snaplog trace] to dump web100 kernel MIB variables' values written in a fixed time (default is 5 msec) increments during the [NDTTestMethodology#C2S_Throughput_Test C2S throughput test] for the newly created connection. This snaplog trace dump is only started when the `--snaplog` option is set.

In the next step the client starts a 10 seconds throughput test using the newly created connection. The NDT client sends packets as fast as possible (i.e. without any delays) during the test. These packets are written using the 8192 Byte buffer containing a pre-generated pseudo random data (including only US-ASCII printable characters).

When the 10 seconds throughput test is over, the server sends its calculated throughput value to the client. The throughput value is calculated by taking the received bytes over the duration of the test. This value, in Bps, is then converted to kbps. This can be shown by the following formula:
{{{
THROUGHPUT_VALUE = (RECEIVED_BYTES / TEST_DURATION_SECONDS) * 8 / 1000
}}}

==== Known Limitations (C2S Throughput Test) ====

A 10 second test may not be enough time for TCP to reach a steady-state on a high bandwidth, high latency link.

=== S2C Throughput Test ===

The S2C throughput test tests the achievable network throughput from the server to the client by performing a 10 seconds memory-to-memory data transfer.

A detailed description of all of the S2C protocol messages can be found in the [NDTProtocol#S2C_throughput_test NDT Protocol document].

As a first step the server binds a new port and notifies the client about this port number.

Next, the client connects to the server's newly bound port. When the connection is successfully established, the server initializes the following routines:
 * libpcap routines to perform packet trace used by the [NDTTestMethodology#Bottleneck_Link_Detection Bottleneck Link Detection] algorithm.
 * [NDTDataFormat#tcpdump_trace tcpdump trace] to dump all packets sent during the [NDTTestMethodology#S2C_Throughput_Test S2C throughput test] on the newly created connection. This tcpdump trace dump is only started when the `-t, --tcpdump` options are set.
 * [NDTDataFormat#web100_snaplog_trace web100 snaplog trace] to dump web100 kernel MIB variables' values written in a fixed time (default is 5 msec) increments during the [NDTTestMethodology#S2C_Throughput_Test S2C throughput test] for the newly created connection. This snaplog trace dump is only started when the `--snaplog` option is set.

In the next step the server starts a 10 seconds throughput test using the newly created connection. The NDT server sends packets as fast as possible (i.e. without any delays) during the test. These packets are written using the 8192 Byte buffer containing a pre-generated pseudo random data (including only US-ASCII printable characters).

When the 10 seconds throughput test is over, the server sends to the client its calculated throughput value, amount of unsent data in the socket send queue and overall number of sent bytes. The throughput value is calculated by taking the transmitted bytes over the duration of the test. This value, in Bps, is then converted to kbps. This can be shown by the following formula:
{{{
THROUGHPUT_VALUE = (TRANSMITTED_BYTES / TEST_DURATION_SECONDS) * 8 / 1000
}}}

Additionally, at the end of the S2C throughput test, the server also takes a web100 snapshot and sends all the web100 data variables to the client.

==== Known Limitations (S2C Throughput Test) ====

A 10 second test may not be enough time for TCP to reach a steady-state on a high bandwidth, high latency link.

== Specific detection algorithms ==

All of the following detection algorithms are run during the [NDTTestMethodology#S2C_Throughput_Test S2C throughput test]. This means, that the NDT server is the sender and the client is the receiver during all these heuristics. The only exception is the [NDTTestMethodology#Bottleneck_Link_Detection Bottleneck Link Detection] algorithm, which observes all test traffic during both the C2S and the S2C throughput tests.

The detection algorithms were created based on the specially developed analytical model of the TCP connection. The specific heuristics were then tuned during the tests performed in the laboratory and the real LAN, MAN and WAN environments.

=== Bottleneck Link Detection ===

NDT tries to find the answer to the question "What is the link in the end-to-end path with the smallest capacity?" by doing the following:
 * monitoring packet arrival times using libpcap routine (all test traffic during both the C2S and the S2C throughput tests is monitored)
 * using TCP dynamics to analyze packet pairs (i.e. compare two subsequent packets on the same connection; for example if 4 packets are received (a, b, c and d), then all subsequent packet pairs are analyzed: a-b, b-c and c-d)
 * quantizing results into link type bins

NDT uses packet dispersion techniques (i.e. it measures the inter-packet arrival times for all data and ACK packets sent or received during both the C2S and the S2C throughput tests). It also knows the packet size, so it can calculate the speed for each pair of packets sent or received and quantize the results into the link type bins.

The speed calculation is done using the following formula:
{{{
bits/time
}}}

where:
 * *bits* - the number of bits in the current packet
 * *time* - the time in microseconds from the last packet arrival

It means that the calculated speed is in mbits/second. The results are then quantized into the following bins:

 * 0 < calculated speed (mbits/second) <= 0.01 - *RTT*
 * 0.01 < calculated speed (mbits/second) <= 0.064 - *Dial-up Modem*
 * 0.064 < calculated speed (mbits/second) <= 1.5 - *Cable/DSL modem*
 * 1.5 < calculated speed (mbits/second) <= 10 - *10 Mbps Ethernet or !WiFi 11b subnet*
 * 10 < calculated speed (mbits/second) <= 40 - *45 Mbps T3/DS3 or !WiFi 11 a/g subnet*
 * 40 < calculated speed (mbits/second) <= 100 - *100 Mbps Fast Ethernet subnet*
 * 100 < calculated speed (mbits/second) <= 622 - *a 622 Mbps OC-12 subnet*
 * 622 < calculated speed (mbits/second) <= 1000 - *1.0 Gbps Gigabit Ethernet subnet*
 * 1000 < calculated speed (mbits/second) <= 2400 - *2.4 Gbps OC-48 subnet*
 * 2400 < calculated speed (mbits/second) <= 10000 - *10 Gbps 10 Gigabit Ethernet/OC-192 subnet*
 * bits cannot be determined - *Retransmissions* (this bin counts the duplicated or invalid packets and does not denote a real link type)
 * otherwise - ?

The detected link type is indicated by the bin with the most packet pairs in it.

==== Known limitations (Bottleneck Link Detection) ====

The Bottleneck Link Detection assumes that packet coalescing is disabled.

Networking, especially DSL/Cable and !WiFi, have become notedly faster than when the bin boundaries were first defined.

The results are quantized, meaning that the NDT doesn’t recognize fractional link speed (Ethernet, T3, or FastE). It also wouldn’t detect bonded Etherchannel interfaces.

=== Duplex Mismatch Detection ===

Duplex mismatch is a condition whereby the host Network Interface Card (NIC) and building switch port fail to agree on whether to operate at 'half-duplex' or 'full-duplex'. While this failure will have a large impact on application performance, basic network connectivity still exists. This means that normal testing procedures (e.g., ping, traceroute) may report that no problem exists while real applications will run extremely slowly.

NDT contains two heuristics for the duplex mismatch detection. This heuristic was determined by looking at the web100 variables and determining which variables best indicated faulty hardware. The first heuristic detects whether or not the desktop client link has a duplex mismatch condition. The second heuristic is used to discover if an internal network link has a duplex mismatch condition.

The client link duplex mismatch detection uses the following heuristic.

 * The connection spends over 90% of its time in the congestion window limited state.
 * The theoretical maximum throughput over this link is less than 2 Mbps.
 * There are more than 2 packets being retransmitted every second of the test.
 * The connection experienced a transition into the TCP slow-start state.

NDT implements the above heuristic in the following manner:

 * The [NDTTestMethodology#'Congestion_Limited'_state_time_share 'Congestion Limited' state time share] *is greater than 90%*
 * The [NDTTestMethodology#Theoretical_maximum_throughput theoretical maximum throughput] *is greater than 2Mibps*
 * The number of segments transmitted containing at least some retransmitted data *is greater than 2 per second*
 * The maximum slow start threshold, excluding the initial value, *is greater than 0*
 * The cumulative time of the expired retransmit timeouts RTO *is greater than 1% of the total test time*
 * The link type detected by the [NDTTestMethodology#Link_Type_Detection_Heuristics Link Type Detection Heuristics] is not a wireless link
 * The throughput measured during the MID test (with a limited CWND) *is greater than* the throughput measured during the S2C test
 * The throughput measured during the C2S test *is greater than* the throughput measured during the S2C test

The internal network link duplex mismatch detect uses the following heuristic.

 * The measured client to server throughput rate exceeded 50 Mbps.
 * The measured server to client throughput rate is less than 5 Mbps.
 * The connection spent more than 90% of the time in the receiver window limited state.
 * There is less that 1% packet loss over the life of the connection.

NDT implements the above heuristic in the following manner:

 * The throughput measured during the S2C test *is greater than 50 Mbps*
 * The [NDTTestMethodology#Total_send_throughput total send throughput] *is less than 5 Mbps*
 * The [NDTTestMethodology#'Receiver_Limited'_state_time_share 'Receiver Limited' state time share] *is greater than 90%*
 * The [NDTTestMethodology#Packet_loss packet loss] *is less than 1%*

==== Known issues/limitations (Duplex Mismatch Detection) ====

The client link duplex mismatch heuristic does not work with multiple simultaneous tests. In order to enable this heuristic, the multi-test mode must be disabled (so the `-m, --multiple` options cannot be set).

<font color="red">NDT does not appear to implement the heuristic correctly.</font> The condition "The link type detected by the [NDTTestMethodology#Link_Type_Detection_Heuristics Link Type Detection Heuristics] is not a wireless link" is always fulfilled, because the Duplex Mismatch Detection heuristic is run before the Link Type Detection heuristic.

The difference between the S2C throughput (> 50 Mbps) and the [NDTTestMethodology#Total_send_throughput total send throughput] (< 5 Mbps) is incredibly big, so it looks like a bug in the formula.

=== Link Type Detection Heuristics ===

The following link type detection heuristics are run only when there is no duplex mismatch condition detected and the [NDTTestMethodology#Total_send_throughput total send throughput] is the same or smaller than the [NDTTestMethodology#Theoretical_maximum_throughput theoretical maximum throughput] (which is an expected situation).

==== DSL/Cable modem ====

The link is treated as a DSL/Cable modem when the NDT Server isn't a bottleneck and the [NDTTestMethodology#Total_send_throughput total send throughput] is less than 2 Mbps and less than the [NDTTestMethodology#Theoretical_maximum_throughput theoretical maximum throughput].

This means that all of the following conditions should be met:
 * The cumulative time spent in the 'Sender Limited' state *is less than 0.6 ms*
 * The number of transitions into the 'Sender Limited' state *is 0*
 * The [NDTTestMethodology#Total_send_throughput total send throughput] *is less than 2 Mbps*
 * The [NDTTestMethodology#Total_send_throughput total send throughput] *is less than* [NDTTestMethodology#Theoretical_maximum_throughput theoretical maximum throughput]

===== Known issues (DSL/Cable modem detection heuristic) =====

<font color="red">The [NDTTestMethodology#DSL/Cable_modem DSL/Cable modem] heuristic appears to be broken now because the DSL/Cable modems commonly go above 2Mbps nowadays.</font>

==== IEEE 802.11 (!WiFi) ====

The link is treated as a wireless one when the [NDTTestMethodology#DSL/Cable_modem DSL/Cable modem] is not detected, the NDT Client is a bottleneck and the [NDTTestMethodology#Total_send_throughput total send throughput] is less than 5 Mbps but the [NDTTestMethodology#Theoretical_maximum_throughput theoretical maximum throughput] is greater than 50 Mibps.

This means that all of the following conditions should be met:
 * The heuristic for DSL/Cable modem link *gives negative results*
 * The cumulative time spent in the 'Sender Limited' state *is 0 ms*
 * The [NDTTestMethodology#Total_send_throughput total send throughput] *is less than 5 Mbps*
 * The [NDTTestMethodology#Theoretical_maximum_throughput theoretical maximum throughput] *is greater than 50 Mibps*
 * The number of transitions into the 'Receiver Limited' state *is the same* as the number of transitions into the 'Congestion Limited' state
 * The [NDTTestMethodology#'Receiver_Limited'_state_time_share 'Receiver Limited' state time share] *is greater than 90%*

==== Ethernet link (Fast Ethernet) ====

The link is treated as an Ethernet link (Fast Ethernet) when the !WiFi and DSL/Cable modem are not detected, the [NDTTestMethodology#Total_send_throughput total send throughput] is between 3 and 9.5 Mbps and the connection is very stable.

This means that all of the following conditions should be met:
 * The heuristics for !WiFi and DSL/Cable modem links *give negative results*
 * The [NDTTestMethodology#Total_send_throughput total send throughput] *is less than 9.5 Mbps*
 * The [NDTTestMethodology#Total_send_throughput total send throughput] *is greater than 3 Mbps*
 * The S2C throughput test measured *is less than 9.5 Mbps*
 * The [NDTTestMethodology#Packet_loss packet loss] *is less than 1%*
 * The [NDTTestMethodology#Packets_arriving_out_of_order out of order packets proportion] *is less than 35%*

=== Faulty Hardware Link Detection ===
NDT uses the following heuristic to determine whether or not faulty hardware, like a bad cable, is impacting performance. This heuristic was determined by looking at the web100 variables and determining which variables best indicated faulty hardware.

 * The connection is losing more than 15 packets per second.
 * The connection spends over 60% of the time in the congestion window limited state.
 * The packet loss rate is less than 1% of the packets transmitted. While the connection is losing a large number of packets per second (test 1) the total number of packets transferred during the test is extremely small so the percentage of retransmitted packets is also small value of packet loss rate.
 *  The connection entered the TCP slow-start state.

NDT implements the above heuristic in the following manner:
 * The [NDTTestMethodology#Packet_loss packet loss] multiplied by 100 and divided by the [NDTTestMethodology#Total_test_time total test time] in seconds *is greater than 15*
 * The [NDTTestMethodology#'Congestion_Limited'_state_time_share 'Congestion Limited' state time share] divided by the [NDTTestMethodology#Total_test_time total test time] in seconds *is greater than 0.6*
 * The [NDTTestMethodology#Packet_loss packet loss] *is less than 1%*
 * The maximum slow start threshold, excluding the initial value, *is greater than 0*

==== Known issues (Faulty Hardware Link Detection) ====

<font color="red">NDT does not appear to implement the heuristic correctly.</font> Instead of taking the total number of lost packets, and dividing by the test duration to calculate the packet per second loss rate, the loss rate is multiplied times 100. Since the "The [NDTTestMethodology#Packet_loss packet loss]" is less than 1%, then the packet loss multiplied by 100 and divided by the total test time in seconds is less than 1.

=== Full/Half Link Duplex Setting ===

NDT has a heuristic to detect a half-duplex link in the path. This heuristic was determined by looking at the web100 variables and determining which variables best indicated a half-duplex link.

NDT looks for a connection that toggles rapidly between the sender buffer limited and receiver buffer limited states. However, even though the connection toggles into and out of the sender buffer limited state numerous times, it does not remain in this state for long periods of time as over 95% of the time is spent in the receiver buffer limited state

NDT implements the above heuristic in the following manner:

 * The [NDTTestMethodology#'Receiver_Limited'_state_time_share 'Receiver Limited' state time share] *is greater than 95%*
 * The number of transitions into the 'Receiver Limited' state *is greater than 30 per second*
 * The number of transitions into the 'Sender Limited' state *is greater than 30 per second*

=== Normal Congestion Detection ===

A normal congestion is detected when the connection is congestion limited a non-trivial percent of the time, there isn't a duplex mismatch detected and the NDT Client's receive window isn't the limiting factor.

This means that all of the following conditions should be met:

 * The [NDTTestMethodology#'Congestion_Limited'_state_time_share 'Congestion Limited' state time share] *is greater than 2%*
 * The duplex mismatch condition heuristic *gives negative results*
 * The maximum window advertisement received *is greater than* the maximum congestion window used during Slow Start

=== Firewall Detection ===

A firewall is detected when the connection to the ephemeral port was unsuccessful in the specified time. The results for the server are independent from the results for the client.

Please note, that the NDT states that the node is only *probably* behind a firewall. The connection can be unsuccessful from a variety of other reasons.

Moreover, if there is a connection and the pre-defined string is properly transferred, then there is also only *probably* no firewall on the end-to-end path (technically there still could be a firewall with a range of opened ports or a special rules that allowed this one particular connection to the ephemeral port).

=== NAT Detection ===

The Network Address Translation (NAT) box is detected by the comparison of the client/server IP addresses visible from the server and the client boxes.

When the server IP address visible to the client is different from the one known to the server itself, then the NAT box is modifying the server's IP address.

Analogically, when the client IP address visible to the server is different from the one known to the client itself, then the NAT box is modifying the client's IP address.

=== MSS Modifications ===

NDT checks packet size preservation by comparing the final value of the MSS variable in the MID test (the NDT Server sets the MSS value to 1456 on the listening socket before the NDT Client connects to it; the final value of the MSS variable is read after the NDT Client connects).

When this variable's value is different than 1456, then the network middlebox had to change it during the test.

When this variable's value is 1456, then it means that the packet size is preserved End-to-End.

== Computed variables ==

=== Total test time ===

The total test time is the total time used by the S2C throughput test.

The total test time is computed using the following formula:

{{{
SndLimTimeRwin + SndLimTimeCwnd + SndLimTimeSender
}}}

where:
 * *!SndLimTimeRwin* - The cumulative time spent in the 'Receiver Limited' state during the S2C throughput test
 * *!SndLimTimeCwnd* - The cumulative time spent in the 'Congestion Limited' state during the S2C throughput test
 * *!SndLimTimeSender* - The cumulative time spent in the 'Sender Limited' state during the S2C throughput test

The total test time is kept in microseconds.

=== Total send throughput ===

The total send throughput is the total amount of data (including retransmits) sent by the NDT Server to the NDT Client in the S2C throughput test.

The total send throughput is computed using the following formula:

{{{
DataBytesOut / TotalTestTime * 8
}}}

where:
 * *!DataBytesOut* - The number of octets of data contained in transmitted segments, including retransmitted data.
 * *!TotalTestTime* - [NDTTestMethodology#Total_test_time Total test time]

The total send throughput is kept in Mbps (because [NDTTestMethodology#Total_test_time Total test time] is kept in microseconds).

=== Packet loss ===

The packet loss is the percentage of the lost packets during the S2C throughput test.

The packet loss proportion is computed using the following formula:

{{{
CongestionSignals/PktsOut
}}}

where:
 * *!CongestionSignals* - The number of multiplicative downward congestion window adjustments due to all forms of congestion signals (this roughly correspond to a single lost packet)
 * *!PktsOut* - The total number of segments sent

To avoid possible division by zero, the NDT sets the packet loss percentages to the following values when the `CongestionSignals` is 0:
 * *0.0000000001* - if a link type detected by the Bottleneck Link Detection algorithm using the *Client --> Server data packets*' inter-packet arrival times is faster than a 100 Mbps Fast Ethernet subnet.
 * *0.000001* - otherwise

=== Packets arriving out of order ===

The packets arriving out of order is the percentage of the duplicated packets during the S2C throughput test.

The out of order packets proportion is computed using the following formula:

{{{
DupAcksIn/AckPktsIn
}}}

where:
 * *!DupAcksIn* - The number of duplicate ACKs received (this roughly correspond to a single out-of-order packet when the TCP is forcing retries of such packets)
 * *!AckPktsIn* - The number of non-duplicate ACKs received

=== Average round trip time (Latency/Jitter) ===

The average round trip time is computed using the following formula:

{{{
SumRTT/CountRTT
}}}

where:
 * *SumRTT* - The sum of all sampled round trip times
 * *CountRTT* - The number of round trip time samples

The average round trip time is kept in milliseconds.

=== Theoretical maximum throughput ===

The theoretical maximum throughput is computed using the following formula:

{{{
(CurrentMSS / (AvgRTTSec * sqrt(PktsLoss))) * 8 / 1024 / 1024
}}}

where:
 * *CurrentMSS* - The current maximum segment size (MSS), in octets
 * *AvgRTTSec* - [NDTTestMethodology#Average_round_trip_time_(Latency/Jitter) Average round trip time (Latency/Jitter)] in seconds
 * *!PktsLoss* - [NDTTestMethodology#Packet_loss Packet loss]

The theoretical maximum throughput is kept in Mibps.

The above theoretical maximum throughput comes from the matthis equation ([http://www.psc.edu/networking/papers/model_ccr97.ps]):

{{{
Rate < (MSS/RTT)*(1 / sqrt(p))
}}}

where p is the loss probability.

==== Known issues (Theoretical maximum throughput) ====

The theoretical maximum throughput should be computed to receive Mbps instead of Mibps. This is the only variable in the NDT that is kept in Mibps, so it might lead to the inconsistent results when comparing it with the other values.

=== 'Congestion Limited' state time share ===

The 'Congestion Limited' state time share is the percentage of the time that the NDT Server was limited in sending due to the congestion window.

The 'Congestion Limited' state time share is computed using the following formula:

{{{
SndLimTimeCwnd/TotalTestTime
}}}

where:
 * *!SndLimTimeCwnd* - The cumulative time spent in the 'Congestion Limited' state
 * *!TotalTestTime* - [NDTTestMethodology#Total_test_time Total test time]

=== 'Receiver Limited' state time share ===

The 'Receiver Limited' state time share is the percentage of the time that the NDT Server was limited in sending due to the NDT Client's receive window.

The 'Receiver Limited' state time share is computed using the following formula:

{{{
SndLimTimeRwin/TotalTestTime
}}}

where:
 * *!SndLimTimeRwin* - The cumulative time spent in the 'Receiver Limited' state
 * *!TotalTestTime* - [NDTTestMethodology#Total_test_time Total test time]

=== 'Sender Limited' state time share ===

The 'Sender Limited' state time share is the percentage of the time that the NDT Server was limited in sending due to its own fault.

The 'Sender Limited' state time share is computed using the following formula:

{{{
SndLimTimeSender/TotalTestTime
}}}

where:
 * *!SndLimTimeSender* - The cumulative time spent in the 'Sender Limited' state
 * *!TotalTestTime* - [NDTTestMethodology#Total_test_time Total test time]

== Known issues/limitations ==

Some issues/limitations have been found in the NDT regarding the following areas:
 * [NDTTestMethodology#Known_issues_(Middlebox_Test) Middlebox Test]
 * [NDTTestMethodology#Known_limitations_(Bottleneck_Link_Detection) Bottleneck Link Detection]
 * [NDTTestMethodology#Known_issues/limitations_(Duplex_Mismatch_Detection) Duples Mismatch Detection]
 * [NDTTestMethodology#Known_issues_(DSL/Cable_modem_detection_heuristic) DSL/Cable modem detection heuristic]
 * [NDTTestMethodology#Known_issues_(Faulty_Hardware_Link_Detection) Faulty Hardware Link Detection]
 * [NDTTestMethodology#Known_issues_(Theoretical_maximum_throughput) Theoretical maximum throughput]
